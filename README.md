# goemotion-multilabel-analysis-and-distilbert-optimization-GoEmotions-ONNX

# GoEmotions Multi-Label Emotion Classification with DistilBERT, LoRA, Optuna, and ONNX

This repository presents a complete deep learning pipeline for multi-label emotion classification using the [GoEmotions](https://github.com/google-research/goemotions) dataset. All modeling and training steps are implemented in **pure PyTorch**, without relying on HuggingFace's Trainer or pipeline. HuggingFace Transformers are used only for loading pretrained models and tokenizers.

---

## ğŸ“‚ Repository Structure
ğŸ“ goemotion-distilbert-multilabel-optuna-onnx
â”œâ”€â”€ 1_analysis_goemotion_multilabel.ipynb # EDA & label binarization
â”œâ”€â”€ 2_distilbert_lora_multilabel_optuna.ipynb # Custom PyTorch training with LoRA and Optuna
â””â”€â”€ (Coming Soon) 3_export_onnx_distilbert.ipynb # ONNX export & benchmarking

---

## âœ… Highlights

- ğŸ” Multi-label classification using **DistilBERT**
- ğŸ§  Custom model and training loop using **PyTorch**
- ğŸ”§ Parameter-efficient fine-tuning using **LoRA**
- ğŸ”¬ Hyperparameter optimization with **Optuna**
- ğŸ“Š Comprehensive evaluation with F1, precision, recall
- ğŸš€ Planned: Export and inference using **ONNX**

---

## ğŸ“Œ Why This Project is Different

- **No HuggingFace Trainer used** â€“ All training logic is implemented with native PyTorch, offering full control.
- **LoRA** is implemented to reduce training time and memory.
- **Optuna** is used to automatically find the best hyperparameters.
- Designed for reproducibility and adaptation to other multi-label tasks.

---

## ğŸ“ˆ Evaluation Metrics

- Micro-averaged and macro-averaged precision, recall, and F1
- Label-wise classification report
- Samples-averaged accuracy for multilabel prediction

---

## ğŸ“Š Preliminary Evaluation Summary

**Test Set Results (Before Final Tuning & ONNX):**
- **Micro F1-score:** `0.54`
- **Macro F1-score:** `0.37`
- **Weighted F1-score:** `0.50`
- **Samples Accuracy:** `0.47`

**Top Performing Emotions:**
- `Gratitude` â€” F1: **0.91**
- `Amusement` â€” F1: **0.81**
- `Love` â€” F1: **0.80**
- `Admiration` â€” F1: **0.65**
- `Neutral` â€” F1: **0.62**

**Challenging Emotions (due to imbalance or label confusion):**
- `Grief`, `Relief`, `Nervousness`, `Realization` â€” F1 near `0.00`

> ğŸ“Œ Note: These are early results using default hyperparameters and a basic LoRA setup. Further tuning with Optuna and ONNX optimization is in progress.

## âš™ï¸ Tech Stack

- PyTorch (`torch`)
- HuggingFace `transformers` (only for model/tokenizer loading)
- peft (for efficiency tunning using LoRA)
- Optuna (for tuning learning rate, LoRA rank, etc.)
- scikit-learn
- Pandas / Seaborn / Matplotlib
- ONNX (planned for export)

---

## ğŸ“Š Planned Work

- âœ… Finish model training with best Optuna parameters
- âœ… Generate and save classification report
- ğŸ”œ Export best model to **ONNX** for lightweight deployment
- ğŸ”œ Benchmark ONNX inference (CPU/GPU)

---

## ğŸ“¬ Feedback & Collaboration

Open an issue or fork the repo if youâ€™re interested in collaborating, improving LoRA handling, or helping with ONNX deployment.

---

â­ If this repo helps your research or project, please consider giving it a star!
